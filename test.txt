Problem Statement
Imagine you are working as a data scientist at a home electronics company which manufactures state of the art smart televisions. You want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.

The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:

Thumbs up:  Increase the volume
Thumbs down: Decrease the volume
Left swipe: 'Jump' backwards 10 seconds
Right swipe: 'Jump' forward 10 seconds  
Stop: Pause the movie
 
Each video is a sequence of 30 frames (or images). 

Understanding the Dataset
The training data consists of a few hundred videos categorised into one of the five classes. Each video (typically 2-3 seconds long) is divided into a sequence of 30 frames(images). These videos have been recorded by various people performing one of the five gestures in front of a webcam - similar to what the smart TV will use.


The data is in a folder 'Project_data' and contains a 'train' and a 'val' folder with two CSV files for the two folders. These folders are in turn divided into subfolders where each subfolder represents a video of a particular gesture. Each subfolder, i.e. a video, contains 30 frames (or images). Note that all images in a particular video subfolder have the same dimensions but different videos may have different dimensions. Specifically, videos have two types of dimensions - either 360x360 or 120x160 (depending on the webcam used to record the videos). Hence, you will need to do some pre-processing to standardise the videos. 

 

Each row of the CSV file represents one video and contains three main pieces of information - the name of the subfolder containing the 30 images of the video, the name of the gesture and the numeric label (between 0-4) of the video.

Sample train.csv file contents are below:
WIN_20180925_17_08_43_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_18_28_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_18_56_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_19_51_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_20_14_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_21_28_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_22_55_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_24_49_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_26_25_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_26_51_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_27_40_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_29_52_Pro_Left_Swipe_new;Left_Swipe_new;0
WIN_20180925_17_33_08_Pro_Left_Swipe_new;Left_Swipe_new;0


Your task is to train a model on the 'train' folder which performs well on the 'val' folder as well. For analysing videos using neural networks, two types of architectures are used commonly. One is the standard CNN + RNN architecture in which you pass the images of a video through a CNN which extracts a feature vector for each image, and then pass the sequence of these feature vectors through an RNN. The other popular architecture used to process videos is a natural extension of CNNs - a 3D convolutional network


Convolutions + RNN
The conv2D network will extract a feature vector for each image, and a sequence of these feature vectors is then fed to an RNN-based network. The output of the RNN is a regular softmax (for a classification problem such as this one).

 

3D Convolutional Network, or Conv3D
3D convolutions are a natural extension to the 2D convolutions you are already familiar with. Just like in 2D conv, you move the filter in two directions (x and y), in 3D conv, you move the filter in three directions (x, y and z). In this case, the input to a 3D conv is a video (which is a sequence of 30 RGB images). If we assume that the shape of each image is 100x100x3, for example, the video becomes a 4-D tensor of shape 100x100x3x30 which can be written as (100x100x30)x3 where 3 is the number of channels. Hence, deriving the analogy from 2-D convolutions where a 2-D kernel/filter (a square filter) is represented as (fxf)xc where f is filter size and c is the number of channels, a 3-D kernel/filter (a 'cubic' filter) is represented as (fxfxf)xc (here c = 3 since the input images have three channels). This cubic filter will now '3D-convolve' on each of the three channels of the (100x100x30) tensor.

 

As an example, let's calculate the output shape and the number of parameters in a Conv3D with an example of a video having 7 frames. Each image is an RGB image of dimension 100x100x3. Here, the number of channels is 3.

 

The input (video) is then 7 images stacked on top of each other, so the shape of the input is (100x100x7)x3, i.e (length x width x number of images) x number of channels. Now, let's use a 3-D filter of size 2x2x2. This is represented as (2x2x2)x3 since the filter has the same number of channels as the input (exactly like in 2D convs).

 

Now let's perform a 3D convolution using a (2x2x2) filter on a (100x100x7) matrix (without any padding and stride = 1). You know that the output size after convolution is given by: 

 

n
e
w
_
d
i
m
e
n
s
i
o
n
=
o
l
d
_
d
i
m
e
n
s
i
o
n
−
f
i
l
t
e
r
_
s
i
z
e
+
2
∗
p
a
d
d
i
n
g
s
t
r
i
d
e
+
1

 

In 3D convolutions, the filter convolves in three directions (exactly like it does in two dimensions in 2D convs), so you can extend this formula to the third dimension as well. You get the output shape as:

(
100
−
2
1
+
1
,
100
−
2
1
+
1
,
7
−
2
1
+
1
)
 = (99,99,6)

 

Thus, the output shape after performing a 3D conv with one filter is (99x99x6). Now, if we do (say) 24 such 3D convolutions, the output shape will become (99x99x6)x24. Hence, the new number of channels for the next Conv3D layer becomes 24. This is very similar to what happens in conv2D.

 

Now let's calculate the number of trainable parameters if the input shape is (100x100x7)x3 and it is convolved with 24 3D filters of size (2x2x2) each, expressed as (2x2x2)x3 to give an output of shape (99x99x6)x24. Each filter will have 2x2x2 = 8 trainable parameters for each of the 3 channels. Also, here we consider that there is one bias per filter. Hence, each filter has 8x3 + 1  = 25 trainable parameters. For 24 such filters, we get 25x24 = 600 trainable parameters.

 

Please note here that the order in which the dimensions are specified is different in the starter code provided to you which can be figured out by looking at the custom generator code 


Now create 2 models and show the python code



-----

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LSTM, TimeDistributed, Dropout

def create_cnn_rnn_model(input_shape=(30, 100, 100, 3), num_classes=5):
    model = Sequential()
    
    # TimeDistributed CNN
    model.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), input_shape=input_shape))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')))
    model.add(TimeDistributed(MaxPooling2D((2, 2))))
    model.add(TimeDistributed(Flatten()))
    
    # RNN
    model.add(LSTM(128, return_sequences=False))
    model.add(Dropout(0.5))
    
    # Fully Connected Layer
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

cnn_rnn_model = create_cnn_rnn_model()
cnn_rnn_model.summary()


----

from tensorflow.keras.layers import Conv3D, MaxPooling3D, GlobalAveragePooling3D

def create_conv3d_model(input_shape=(30, 100, 100, 3), num_classes=5):
    model = Sequential()
    
    # Conv3D layers
    model.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape))
    model.add(MaxPooling3D((2, 2, 2)))
    model.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))
    model.add(MaxPooling3D((2, 2, 2)))
    
    # Global Average Pooling
    model.add(GlobalAveragePooling3D())
    
    # Fully Connected Layers
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
    
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

conv3d_model = create_conv3d_model()
conv3d_model.summary()


------


Key Points in the Models
Input Shape:

For CNN + RNN: (30, height, width, channels) where 30 is the number of frames.
For Conv3D: (frames, height, width, channels) with similar dimensions.
CNN: Extracts spatial features for each frame in CNN + RNN.

LSTM: Processes temporal features in the CNN + RNN model.

3D Convolution: Captures spatiotemporal features directly in the Conv3D model.

Compilation: Both models are compiled using the Adam optimizer with a categorical cross-entropy loss suitable for multi-class classification.
